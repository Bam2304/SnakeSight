{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55497190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to remove images smaller than 100x100 pixels\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "def remove_small_images(folder_path, min_width=100, min_height=100):\n",
    "    \"\"\"Remove images that are smaller than the specified dimensions\"\"\"\n",
    "    removed_count = 0\n",
    "    error_count = 0\n",
    "    total_count = 0\n",
    "    removed_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                total_count += 1\n",
    "                \n",
    "                try:\n",
    "                    with Image.open(file_path) as img:\n",
    "                        width, height = img.size\n",
    "                        \n",
    "                        if width < min_width or height < min_height:\n",
    "                            img.close()\n",
    "                            time.sleep(0.1)\n",
    "                            \n",
    "                            for attempt in range(3):\n",
    "                                try:\n",
    "                                    os.remove(file_path)\n",
    "                                    removed_count += 1\n",
    "                                    removed_files.append(f\"{file} ({width}x{height})\")\n",
    "                                    print(f\"Removed: {file_path} - Size: {width}x{height}\")\n",
    "                                    break\n",
    "                                except PermissionError:\n",
    "                                    if attempt < 2:\n",
    "                                        time.sleep(0.5)\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        print(f\"Permission denied: {file_path} - Could not remove after 3 attempts\")\n",
    "                                        error_count += 1\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    error_count += 1\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'total_images': total_count,\n",
    "        'removed_count': removed_count,\n",
    "        'error_count': error_count,\n",
    "        'removed_files': removed_files\n",
    "    }\n",
    "\n",
    "snake_folder = \"Snake Images for Model\"\n",
    "print(f\"Scanning {snake_folder} for images smaller than 100x100 pixels...\\n\")\n",
    "\n",
    "stats = remove_small_images(snake_folder, min_width=100, min_height=100)\n",
    "\n",
    "print(f\"\\n=== CLEANUP SUMMARY ===\")\n",
    "print(f\"Total images processed: {stats['total_images']}\")\n",
    "print(f\"Images removed: {stats['removed_count']}\")\n",
    "print(f\"Errors encountered: {stats['error_count']}\")\n",
    "print(f\"Remaining images: {stats['total_images'] - stats['removed_count']}\")\n",
    "\n",
    "if stats['removed_files']:\n",
    "    print(f\"\\nRemoved files (first 10):\")\n",
    "    for file_info in stats['removed_files'][:10]:\n",
    "        print(f\"  - {file_info}\")\n",
    "    if len(stats['removed_files']) > 10:\n",
    "        print(f\"  ... and {len(stats['removed_files']) - 10} more files\")\n",
    "else:\n",
    "    print(\"\\nNo images were removed - all images meet the minimum size requirement!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1124b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow keras keras-preprocessing keras-applications split-folders opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8867990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import splitfolders\n",
    "import cv2\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ca1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "input_folder = \"Snake Images for Model\"\n",
    "output_folder = \"split_data\"\n",
    "\n",
    "splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(0.7, 0.2, 0.1))\n",
    "\n",
    "IMG_HEIGHT = 160\n",
    "IMG_WIDTH = 160\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def simple_preprocessing(img):\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=simple_preprocessing,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.12, # changed from 0.1 to 0.12\n",
    "    height_shift_range=0.12, # changed from 0.1 to 0.12\n",
    "    shear_range=0.12, # changed from 0.1 to 0.12\n",
    "    zoom_range=0.12, # changed from 0.1 to 0.12\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=simple_preprocessing)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=simple_preprocessing)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    f'{output_folder}/train',\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    f'{output_folder}/val',\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    f'{output_folder}/test',\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "num_classes = len(train_generator.class_indices)\n",
    "print(f\"Number of snake classes: {num_classes}\")\n",
    "print(\"\\nClass mapping:\")\n",
    "for class_name, class_index in train_generator.class_indices.items():\n",
    "    print(f\"{class_index}: {class_name}\")\n",
    "\n",
    "train_generator.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d59e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progressive training at multiple resolutions\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D\n",
    "from PIL import Image\n",
    "\n",
    "TRAINING_STAGES = [\n",
    "    {\"img_size\": 160, \"epochs\": 25, \"stage_name\": \"Stage 1: 160x160\"},\n",
    "    {\"img_size\": 192, \"epochs\": 15, \"stage_name\": \"Stage 2: 192x192\"},\n",
    "    {\"img_size\": 224, \"epochs\": 15, \"stage_name\": \"Stage 3: 224x224\"},\n",
    "    {\"img_size\": 400, \"epochs\": 40, \"stage_name\": \"Stage 4: 400x400\", \"learning_rate\": 5e-5}\n",
    "]\n",
    "\n",
    "def create_model_for_size(img_size, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=(img_size, img_size, 3)),\n",
    "        \n",
    "        Conv2D(32, (3, 3), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        \n",
    "        Conv2D(64, (3, 3), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        \n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        \n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        \n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        \n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def transfer_weights(source_model, target_model, num_classes):\n",
    "    for source_layer, target_layer in zip(source_model.layers, target_model.layers):\n",
    "        if isinstance(source_layer, tf.keras.layers.InputLayer):\n",
    "            continue\n",
    "        if source_layer.name.startswith('dense') and target_layer.name.startswith('dense'):\n",
    "            if source_layer.get_weights() and target_layer.get_weights():\n",
    "                source_weights = source_layer.get_weights()\n",
    "                target_weights = target_layer.get_weights()\n",
    "                if source_weights[0].shape == target_weights[0].shape:\n",
    "                    target_layer.set_weights(source_weights)\n",
    "        else:\n",
    "            try:\n",
    "                if source_layer.get_weights():\n",
    "                    target_layer.set_weights(source_layer.get_weights())\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "def filter_images_by_size(directory, min_width, min_height):\n",
    "    \"\"\"Filter images in directory to only include those meeting minimum size requirements\"\"\"\n",
    "    valid_images = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with Image.open(file_path) as img:\n",
    "                        width, height = img.size\n",
    "                        if width >= min_width and height >= min_height:\n",
    "                            valid_images.append(file_path)\n",
    "                except:\n",
    "                    pass\n",
    "    return valid_images\n",
    "\n",
    "class AdaptiveLearningRateIncrease(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Custom callback to increase learning rate when loss improvement is too slow\"\"\"\n",
    "    def __init__(self, patience=4, increase_factor=2.0, min_lr=1e-6, max_lr=1e-3, monitor='val_loss'):\n",
    "        super().__init__()\n",
    "        self.patience = patience\n",
    "        self.increase_factor = increase_factor\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.monitor = monitor\n",
    "        self.wait = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.improvement_threshold = 0.001  # Minimum improvement to reset patience\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_loss = logs.get(self.monitor)\n",
    "        current_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        \n",
    "        # Check if there's meaningful improvement\n",
    "        if current_loss < self.best_loss - self.improvement_threshold:\n",
    "            self.best_loss = current_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            \n",
    "        # If no improvement for patience epochs and LR can be increased\n",
    "        if self.wait >= self.patience and current_lr < self.max_lr:\n",
    "            new_lr = min(current_lr * self.increase_factor, self.max_lr)\n",
    "            if new_lr > self.min_lr:\n",
    "                tf.keras.backend.set_value(self.model.optimizer.learning_rate, new_lr)\n",
    "                print(f\"\\nEpoch {epoch + 1}: Loss plateau detected. Increasing learning rate from {current_lr:.2e} to {new_lr:.2e}\")\n",
    "                self.wait = 0  # Reset patience after LR increase\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PROGRESSIVE TRAINING: Multi-Resolution CNN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "current_model = None\n",
    "stage_histories = {}\n",
    "\n",
    "for stage_idx, stage_config in enumerate(TRAINING_STAGES):\n",
    "    img_size = stage_config[\"img_size\"]\n",
    "    epochs = stage_config[\"epochs\"]\n",
    "    stage_name = stage_config[\"stage_name\"]\n",
    "    learning_rate = stage_config.get(\"learning_rate\", 1e-4)\n",
    "    \n",
    "    print(f\"\\n{stage_name}\")\n",
    "    print(f\"Image size: {img_size}x{img_size}, Epochs: {epochs}, Learning rate: {learning_rate}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    stage_train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=simple_preprocessing,\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.11,\n",
    "        height_shift_range=0.11,\n",
    "        shear_range=0.11,\n",
    "        zoom_range=0.11,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    stage_val_datagen = ImageDataGenerator(preprocessing_function=simple_preprocessing)\n",
    "    \n",
    "    stage_train_gen = stage_train_datagen.flow_from_directory(\n",
    "        f'{output_folder}/train',\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=16,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    stage_val_gen = stage_val_datagen.flow_from_directory(\n",
    "        f'{output_folder}/val',\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=16,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    stage_model = create_model_for_size(img_size, num_classes)\n",
    "    \n",
    "    if current_model is not None:\n",
    "        print(f\"Transferring weights from {TRAINING_STAGES[stage_idx - 1]['stage_name']}...\")\n",
    "        transfer_weights(current_model, stage_model, num_classes)\n",
    "    \n",
    "    stage_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    if stage_idx == 0:\n",
    "        print(f\"\\nQuick sanity check...\")\n",
    "        sanity_check = stage_model.fit(\n",
    "            stage_train_gen,\n",
    "            epochs=1,\n",
    "            validation_data=stage_val_gen,\n",
    "            verbose=1,\n",
    "            steps_per_epoch=5,\n",
    "            validation_steps=2\n",
    "        )\n",
    "        stage_train_gen.reset()\n",
    "        stage_val_gen.reset()\n",
    "    \n",
    "    print(f\"\\nTraining {stage_name}...\")\n",
    "    \n",
    "    # Define base callbacks\n",
    "    callbacks_list = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=8,  # Increased patience for final stage\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            f'model_stage_{img_size}.keras',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Add different callbacks for final stage vs other stages\n",
    "    if stage_idx == len(TRAINING_STAGES) - 1:  # Final stage (400x400)\n",
    "        print(\"Final stage: Using adaptive learning rate increase callback\")\n",
    "        callbacks_list.append(AdaptiveLearningRateIncrease(\n",
    "            patience=4,\n",
    "            increase_factor=2.0,\n",
    "            min_lr=1e-6,\n",
    "            max_lr=1e-3,\n",
    "            monitor='val_loss'\n",
    "        ))\n",
    "    else:\n",
    "        # Standard ReduceLROnPlateau for earlier stages\n",
    "        callbacks_list.append(tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ))\n",
    "    \n",
    "    history = stage_model.fit(\n",
    "        stage_train_gen,\n",
    "        epochs=epochs,\n",
    "        validation_data=stage_val_gen,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list\n",
    "    )\n",
    "    \n",
    "    stage_histories[stage_name] = history\n",
    "    current_model = stage_model\n",
    "    \n",
    "    print(f\"\\n{stage_name} completed!\")\n",
    "    print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "model = current_model\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROGRESSIVE TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final model input size: {TRAINING_STAGES[-1]['img_size']}x{TRAINING_STAGES[-1]['img_size']}\")\n",
    "print(f\"Model saved as: model_stage_{TRAINING_STAGES[-1]['img_size']}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ef259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot progressive training history across all stages\n",
    "num_stages = len(stage_histories)\n",
    "fig, axes = plt.subplots(2, num_stages, figsize=(6*num_stages, 10))\n",
    "fig.suptitle('Progressive Training: Multi-Resolution CNN', fontsize=16, fontweight='bold')\n",
    "\n",
    "stage_names = list(stage_histories.keys())\n",
    "\n",
    "# Handle case where there's only one stage (axes won't be 2D)\n",
    "if num_stages == 1:\n",
    "    axes = axes.reshape(2, 1)\n",
    "\n",
    "for idx, stage_name in enumerate(stage_names):\n",
    "    history = stage_histories[stage_name]\n",
    "    \n",
    "    ax_acc = axes[0, idx]\n",
    "    ax_acc.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    ax_acc.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    ax_acc.set_title(f'{stage_name}\\nAccuracy')\n",
    "    ax_acc.set_xlabel('Epoch')\n",
    "    ax_acc.set_ylabel('Accuracy')\n",
    "    ax_acc.legend()\n",
    "    ax_acc.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax_loss = axes[1, idx]\n",
    "    ax_loss.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    ax_loss.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    ax_loss.set_title(f'{stage_name}\\nLoss')\n",
    "    ax_loss.set_xlabel('Epoch')\n",
    "    ax_loss.set_ylabel('Loss')\n",
    "    ax_loss.legend()\n",
    "    ax_loss.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "final_size = TRAINING_STAGES[-1]['img_size']\n",
    "print(f\"\\nEvaluating final model ({final_size}x{final_size}) on test set...\")\n",
    "\n",
    "test_datagen_final = ImageDataGenerator(preprocessing_function=simple_preprocessing)\n",
    "test_generator_eval = test_datagen_final.flow_from_directory(\n",
    "    f'{output_folder}/test',\n",
    "    target_size=(final_size, final_size),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_generator_eval, verbose=1)\n",
    "print(f\"\\nFinal Model Test Results ({final_size}x{final_size}):\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "\n",
    "def predict_snake(image_path, model_to_use=model):\n",
    "    final_size = TRAINING_STAGES[-1]['img_size']\n",
    "    img = tf.keras.preprocessing.image.load_img(\n",
    "        image_path, target_size=(final_size, final_size)\n",
    "    )\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, 0)\n",
    "    img_array = simple_preprocessing(img_array)\n",
    "    \n",
    "    predictions = model_to_use.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_class]\n",
    "    \n",
    "    class_names = list(train_generator.class_indices.keys())\n",
    "    predicted_snake = class_names[predicted_class]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Predicted: {predicted_snake}\\nConfidence: {confidence:.2%}')\n",
    "    plt.show()\n",
    "    \n",
    "    top_3_idx = np.argsort(predictions[0])[-3:][::-1]\n",
    "    print(\"\\nTop 3 predictions:\")\n",
    "    for idx in top_3_idx:\n",
    "        print(f\"{class_names[idx]}: {predictions[0][idx]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba3bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test predictions on Testing Snake folder\n",
    "test_folder = \"Testing Snake\"\n",
    "image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif')\n",
    "\n",
    "if os.path.exists(test_folder):\n",
    "    image_files = [f for f in os.listdir(test_folder) if f.lower().endswith(image_extensions)]\n",
    "    \n",
    "    if image_files:\n",
    "        print(f\"Found {len(image_files)} images in {test_folder}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for image_file in image_files:\n",
    "            image_path = os.path.join(test_folder, image_file)\n",
    "            print(f\"\\nTesting: {image_file}\")\n",
    "            print(\"-\" * 60)\n",
    "            predict_snake(image_path)\n",
    "    else:\n",
    "        print(f\"No image files found in {test_folder}\")\n",
    "else:\n",
    "    print(f\"Folder {test_folder} not found\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
